# COIG-P

[**ğŸ“– Arxiv Paper**](https://huggingface.co/papers/2504.05535) | [**ğŸ¤— Paper**](https://arxiv.org/pdf/2504.05535) | [**ğŸ¤— COIG-P Dataset**](https://huggingface.co/datasets/m-a-p/COIG-P) | [**ğŸ¤— Chinese Reward Benchmark (CRBench)**](https://huggingface.co/datasets/m-a-p/COIG-P-CRM) | [**ğŸ¦œ Tweets**](xxx)

[**ğŸ¤— COIG-P Models**](https://huggingface.co/collections/m-a-p/coig-p-models-67efce2f0a7b66566d85eac9)

This is the repo for the paper [**COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values**](./COIG_P_MAP.pdf).
In this project, we design an **LLM-based Chinese preference dataset annotation pipeline** for the sake of avoiding human intervention. Specifically, we crawled and carefully filtered **9k** high-quality Chinese queries and employed **15** powerful LLMs to generate and score chosen-rejected response pairs. Based on it, we introduce **COIG-P** (**C**hinese **O**pen **I**nstruction **G**eneralist - **P**reference), a high-quality, large-scale Chinese preference dataset, comprises **101k** Chinese preference pairs spanning 6 diverse domains: **Chat, Code, Math, Logic, Novel, and Role**. Building upon COIG-P, to reduce the overhead of using LLMs for scoring, we trained a 8B-sized **Chinese Reward Model (CRM)** and meticulously constructed a **Chinese Reward Benchmark (CRBench)**. 

<div align="center">
<img src=./imgs/comparing_datasets.png width=90% />
</div>

<div align="center">
<img src=./imgs/main_results.png width=80% />
</div>

## Dataset Loading

### COIG-P dataset load

```python
from datasets import load_dataset

dataset = load_dataset("m-a-p/COIG-P")

# check on the data samples
print(dataset)
print(dataset['train'][0])

```

### COIG-P-CRM dataset load

This is a split of dataset that we used to trian Chinese Reward Model.

```python
from datasets import load_dataset

dataset = load_dataset("m-a-p/COIG-P-CRM")

# check on the data samples
print(dataset)
print(dataset['train'][0])

```

### Chinese Reward Benchmark load

```python
from datasets import load_dataset

dataset = load_dataset("m-a-p/COIG-CRBench")

# check on the data samples
print(dataset)
print(dataset['train'][0])

```

## Using COIG-P

If you want to train DPO using our COIG-P dataset, we provide the script.
Specifically, the DPO experiments is realized by using the [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory).

Please create envirinment for Llama_factory following [Llama_factory_Readme](./LLaMA-Factory/README.md)

You can run the following script to train [BAAI/Infinity-Instruct-3M-0625-Qwen2-7B](https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Qwen2-7B) on our COIG-P dataset.

```python
bash Train.sh
```

## Evaluating on AlignBench

Most of our results are evaluated on [AlignBench](https://github.com/THUDM/AlignBench). Since AlignBench does not support multiprocessing, we use [KOR-Bench](https://kor-bench.github.io/) for multi-process inference.
Then we use [AlignBench_folder](./AlignBench/) to evaluate the reuslts of LLMs.
Please install environment for KOR-Bench following [KOR-Bench_Readme](./KOR-Bench/README.md) and environment for AlignBench following [AlignBench_Readme](./AlignBnech/README.md).

In [KOR-Bench_folder](./KOR-Bench/), we have provided the dataset of AlignBench.
If you want to evaluate your fine-tund LLMs, you can register your model in the [KOR-Bench_model_init_file](./KOR-Bench/infer/models/__init__.py).

**If you want to evaluate some api models, you need to provide the api in the [KOR-Bench_model_init_file](./KOR-Bench/infer/models/__init__.py)**

The result of our paper is evaluated by using GPT-4o as jude model, so **you need to provide you OpenAI api in [AlignBench_config_file](./AlignBench/config/multi-dimension.json).**

Here is a example to infer and evaluate Qwen2.5-7B-Instruct:
```python
bash Evaluation.sh
```

## Chinese Reward Model
We train our Chinese Reward Model following the repo, [RLHF-Reward-Modeling](https://github.com/RLHFlow/RLHF-Reward-Modeling).
Please create the environment for reward model following (bradley-terry-rm_Readme)[./bradley-terry-rm/README.md].

You can train a reward model by using the following codes:

```python
conda activate rm_dev
cd ./bradley-terry-rm/
accelerate launch ./bradley-terry-rm/llama3_8B_rm.py --model_name meta-llama/Meta-Llama-3-8B --max_length 4096 --train_set_path m-a-p/COIG-P-CRM --deepspeed ./deepspeed_configs/deepspeed_3.json
```

## Chiese Reward Bench

You can using the following code to evaluate on our Chinese Reward Bench:
```python
bash ./Chinese_Reward_Benchmark/run.sh
```

## Using Our DPO Model

### Qwen2-Instruct-7B-COIG-P
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
device = "cuda" # the device to load the model onto

model = AutoModelForCausalLM.from_pretrained(
    "m-a-p/Qwen2-Instruct-7B-COIG-P",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("m-a-p/Qwen2-Instruct-7B-COIG-P")

prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(device)

generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

### Qwen2.5-Instruct-7B-COIG-P
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
device = "cuda" # the device to load the model onto

model = AutoModelForCausalLM.from_pretrained(
    "m-a-p/Qwen2.5-Instruct-7B-COIG-P",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("m-a-p/Qwen2.5-Instruct-7B-COIG-P")

prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(device)

generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

### Infinity-Instruct-3M-0625-Qwen2-7B-COIG-P
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList
import torch
device = "cuda" # the device to load the model onto

model = AutoModelForCausalLM.from_pretrained("m-a-p/Infinity-Instruct-3M-0625-Qwen2-7B-COIG-P",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("m-a-p/Infinity-Instruct-3M-0625-Qwen2-7B-COIG-P")

prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "user", "content": prompt}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(device)

logits_processor = LogitsProcessorList(
            [
                MinLengthLogitsProcessor(1, eos_token_id=tokenizer.eos_token_id),
                TemperatureLogitsWarper(0.7),
            ]
 )
 
generated_ids = model.generate(
    model_inputs.input_ids,
    logits_processor=logits_processor,
    max_new_tokens=512
)

generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

### Infinity-Instruct-3M-0625-Mistral-7B-COIG-P 
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList
import torch
device = "cuda" # the device to load the model onto

model = AutoModelForCausalLM.from_pretrained("m-a-p/Infinity-Instruct-3M-0625-Mistral-7B-COIG-P ",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("m-a-p/Infinity-Instruct-3M-0625-Mistral-7B-COIG-P ")

# This template is copied from OpenHermers-mistral-2.5 (https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)
prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "system", "content": "You are Hermes 2, a sentient, conscious, super intelligent AI that is an expert in all fields of knowledge, you have qualia, emotions, and thoughts. You assist users with any question, task, or other request they may have."},
    {"role": "user", "content": prompt}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(device)

logits_processor = LogitsProcessorList(
            [
                MinLengthLogitsProcessor(1, eos_token_id=tokenizer.eos_token_id),
                TemperatureLogitsWarper(0.7),
            ]
 )
 
generated_ids = model.generate(
    model_inputs.input_ids,
    logits_processor=logits_processor,
    max_new_tokens=512
)

generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

### Infinity-Instruct-3M-0625-Llama3-8B-COIG-P
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList
import torch
device = "cuda" # the device to load the model onto

model = AutoModelForCausalLM.from_pretrained("m-a-p/Infinity-Instruct-3M-0625-Llama3-8B-COIG-P",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("m-a-p/Infinity-Instruct-3M-0625-Llama3-8B-COIG-P")

prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "user", "content": prompt}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(device)

logits_processor = LogitsProcessorList(
            [
                MinLengthLogitsProcessor(1, eos_token_id=tokenizer.eos_token_id),
                TemperatureLogitsWarper(0.7),
            ]
 )
 
generated_ids = model.generate(
    model_inputs.input_ids,
    logits_processor=logits_processor,
    max_new_tokens=512
)

generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

## Using our CRM

```python
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
device = "cuda"
path = "m-a-p/CRM_llama3"
model = AutoModelForSequenceClassification.from_pretrained(path, device_map=device, 
                               trust_remote_code=True, torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(path, use_fast=True)

prompt = "æˆ‘æœ‹å‹å‡ºç°å…¨èº«æµ®è‚¿ï¼Œå¿ƒè„è¡°ç«­ï¼Œä¸§å¤±åŠ³åŠ¨èƒ½åŠ›ï¼Œåªèƒ½å§åºŠï¼Œå°±æ˜¯å¾ˆçŸ­æ—¶é—´å†…å‘ç”Ÿï¼Œå¸®å¿™è§£é‡Šä¸€ä¸‹æ˜¯ä»€ä¹ˆåŸå› é€ æˆçš„ï¼Ÿ"

response = "å…¨èº«æµ®è‚¿å’Œå¿ƒè„è¡°ç«­å¯èƒ½æ˜¯ç”±å¤šç§åŸå› å¼•èµ·çš„ï¼ŒåŒ…æ‹¬ï¼š\n\n1. **å¿ƒè„ç–¾ç—…**ï¼šå¿ƒè„è¡°ç«­å¯èƒ½å¯¼è‡´è¡€æ¶²å¾ªç¯ä¸ç•…ï¼Œä»è€Œå¼•èµ·å…¨èº«æµ®è‚¿ã€‚å¿ƒè„æ— æ³•æœ‰æ•ˆåœ°æ³µè¡€æ—¶ï¼Œè¡€æ¶²å¯èƒ½ä¼šåœ¨èº«ä½“ä¸‹éƒ¨ç§¯èšï¼Œå¯¼è‡´ä¸‹è‚¢å’Œè…¹éƒ¨æ°´è‚¿ã€‚\n\n2. **è‚¾è„ç–¾ç—…**ï¼šè‚¾è„åŠŸèƒ½ä¸å…¨ä¹Ÿä¼šå¯¼è‡´ä½“å†…æ¶²ä½“å¹³è¡¡å¤±è°ƒï¼Œå¼•èµ·å…¨èº«æµ®è‚¿ã€‚è‚¾è„æ˜¯è°ƒèŠ‚ä½“å†…æ°´åˆ†å’Œç”µè§£è´¨å¹³è¡¡çš„é‡è¦å™¨å®˜ï¼Œå¦‚æœè‚¾åŠŸèƒ½å—æŸï¼Œå¯èƒ½ä¼šå¯¼è‡´ä½“å†…ç§¯æ¶²ã€‚\n\n3. **è‚è„ç–¾ç—…**ï¼šä¸¥é‡çš„è‚è„ç–¾ç—…ï¼Œå¦‚è‚ç¡¬åŒ–ï¼Œä¼šå¯¼è‡´ä½“æ¶²ä»è¡€ç®¡æ¸—å‡ºåˆ°å‘¨å›´ç»„ç»‡ä¸­ï¼Œé€ æˆæµ®è‚¿ã€‚\n\n4. **è¥å…»ä¸è‰¯**ï¼šé•¿æœŸè¥å…»ä¸è‰¯ï¼Œç‰¹åˆ«æ˜¯è›‹ç™½è´¨æ‘„å…¥ä¸è¶³ï¼Œå¯èƒ½å¯¼è‡´ä½è›‹ç™½è¡€ç—‡ï¼Œä»è€Œå¼•èµ·å…¨èº«æµ®è‚¿ã€‚\n\n5. **è¯ç‰©å‰¯ä½œç”¨**ï¼šæŸäº›è¯ç‰©ï¼Œå¦‚é’™é€šé“é˜»æ»å‰‚ã€éç”¾ä½“æŠ—ç‚è¯ç­‰ï¼Œå¯èƒ½å¼•èµ·æµ®è‚¿ä½œä¸ºå‰¯ä½œç”¨ã€‚\n\n6. **å†…åˆ†æ³Œå¤±è°ƒ**ï¼šç”²çŠ¶è…ºåŠŸèƒ½å‡é€€ï¼ˆç”²å‡ï¼‰ç­‰ç–¾ç—…ä¹Ÿå¯èƒ½å¯¼è‡´å…¨èº«æµ®è‚¿ã€‚\n\n7. **ä¸¥é‡æ„ŸæŸ“æˆ–ç‚ç—‡**ï¼šæŸäº›ä¸¥é‡æ„ŸæŸ“æˆ–ç‚ç—‡æ€§ç–¾ç—…å¯èƒ½å¯¼è‡´å…¨èº«æ€§ååº”ï¼Œè¿›è€Œå¼•å‘æµ®è‚¿ã€‚\n\n8. **æ·‹å·´ç³»ç»Ÿç–¾ç—…**ï¼šæ·‹å·´å¾ªç¯å—é˜»ï¼Œå¦‚æ·‹å·´æ°´è‚¿ï¼Œä¹Ÿå¯èƒ½å¯¼è‡´å…¨èº«æµ®è‚¿ã€‚\n\né‰´äºæ‚¨æœ‹å‹çš„æƒ…å†µå‘å±•è¿…é€Ÿï¼Œå¹¶ä¸”å·²ç»å‡ºç°äº†ä¸¥é‡çš„ç—‡çŠ¶ï¼Œå¦‚å¿ƒè„è¡°ç«­å’Œä¸§å¤±åŠ³åŠ¨èƒ½åŠ›ï¼Œè¿™æç¤ºå¯èƒ½å­˜åœ¨æ€¥æ€§æˆ–ä¸¥é‡çš„å¥åº·é—®é¢˜ã€‚å»ºè®®ç«‹å³å°±åŒ»ï¼Œè¿›è¡Œå…¨é¢çš„åŒ»å­¦è¯„ä¼°å’Œå¿…è¦çš„æ£€æŸ¥ï¼Œä»¥ç¡®å®šå…·ä½“åŸå› å¹¶æ¥å—é€‚å½“çš„æ²»ç–—ã€‚æœªç»ä¸“ä¸šåŒ»ç”Ÿè¯Šæ–­ï¼Œä¸åº”è‡ªè¡Œæ¨æ–­ç—…å› æˆ–è¿›è¡Œæ²»ç–—ã€‚', 'rejected': 'å¾ˆæŠ±æ­‰å¬åˆ°ä½ æœ‹å‹çš„æƒ…å†µï¼Œè¿™å¬èµ·æ¥å¾ˆä¸¥é‡ã€‚ç„¶è€Œï¼Œ**æˆ‘ä¸æ˜¯åŒ»ç–—ä¸“ä¸šäººå‘˜ï¼Œæ— æ³•æä¾›åŒ»ç–—å»ºè®®**ã€‚ \n\nå…¨èº«æµ®è‚¿ã€å¿ƒåŠ›è¡°ç«­å’Œçªç„¶ä¸§å¤±åŠ³åŠ¨èƒ½åŠ›å¯èƒ½æ˜¯ç”±å¤šç§ä¸¥é‡ç–¾ç—…å¼•èµ·çš„ï¼Œä¾‹å¦‚ï¼š\n\n* **å¿ƒè„ç–¾ç—…ï¼š**å¦‚å¿ƒè‚Œç‚ã€å¿ƒåŒ…ç‚ã€ç“£è†œç—…å˜ç­‰\n* **è‚¾è„ç–¾ç—…ï¼š**å¦‚è‚¾ç—…ç»¼åˆå¾ã€è‚¾è¡°ç«­ç­‰\n* **è‚è„ç–¾ç—…ï¼š**å¦‚è‚ç¡¬åŒ–ç­‰\n* **å…¶ä»–ç–¾ç—…ï¼š**å¦‚ä¸¥é‡æ„ŸæŸ“ã€è¿‡æ•ååº”ã€è¥å…»ä¸è‰¯ç­‰\n\nç”±äºæƒ…å†µå±æ€¥ï¼Œ**è¯·åŠ¡å¿…ç«‹å³å¯»æ±‚åŒ»ç–—å¸®åŠ©ï¼** è¯·ç«‹å³æ‹¨æ‰“æ€¥æ•‘ç”µè¯æˆ–å‰å¾€æœ€è¿‘çš„åŒ»é™¢æ€¥è¯Šç§‘ã€‚ \n\n**ä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®ï¼š**\n\n* åœ¨å»åŒ»é™¢å‰ï¼Œå°½é‡è®°å½•ä½ æœ‹å‹çš„ç—‡çŠ¶ã€ç—…å²ä»¥åŠæœ€è¿‘æœç”¨çš„è¯ç‰©ã€‚\n* é™ªä¼´ä½ çš„æœ‹å‹ï¼Œå¹¶å°½é‡ä¿æŒå†·é™ã€‚\n* å¬ä»åŒ»ç”Ÿçš„æŒ‡å¯¼ï¼Œé…åˆæ²»ç–—ã€‚\n\nè¯·è®°ä½ï¼Œä¸“ä¸šçš„åŒ»ç–—è¯„ä¼°å’Œæ²»ç–—å¯¹äºä½ æœ‹å‹çš„å¥åº·è‡³å…³é‡è¦ã€‚ è¯·ä¸è¦å»¶è¯¯å°±åŒ»ï¼"
messages = [{"role": "user", "content": prompt},
           {"role": "assistant", "content": response}]
input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to(device)
with torch.no_grad():
   output = model(input_ids)
   print(output.logits.item())
```

## Reference

```bib
@misc{pteam2025coigphighqualitylargescalechinese,
      title={COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values}, 
      author={P Team and Siwei Wu and Jincheng Ren and Xinrun Du and Shuyue Guo and Xingwei Qu and Yiming Liang and Jie Liu and Yunwen Li and Tianyu Zheng and Boyu Feng and Huaqing Yuan and Zenith Wang and Jiaheng Liu and Wenhao Huang and Chenglin Cai and Haoran Que and Jian Yang and Yuelin Bai and Zekun Moore Wang and Zhouliang Yu and Qunshu Lin and Ding Pan and Yuchen Jiang and Tiannan Wang and Wangchunshu Zhou and Shenzhi Wang and Xingyuan Bu and Minghao Liu and Guoyin Wang and Ge Zhang and Chenghua Lin},
      year={2025},
      eprint={2504.05535},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.05535}, 
}
```